# -*- coding: utf-8 -*-
"""Capstone_Project_Grp13_EDA_DataPrep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qOOqCBEarmSeWpiWSpLKH0v1lRJ00lbO
"""

#Mount Google Drive

# from google.colab import drive
# drive.mount('/content/drive/')

# Commented out IPython magic to ensure Python compatibility.
#Importing required libraries for the NLP 2 Capstone Project
def processEDAData():

    import string
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    # %matplotlib inline
    import seaborn as sns
    from sklearn.preprocessing import LabelEncoder, OneHotEncoder
    from scipy.stats import chi2_contingency
    import datetime
    from scipy.stats import chi2
    import nltk # Import NLTK
    nltk.download('stopwords') # Downloading stopwords
    nltk.download('punkt') # Downloading tokenizer
    from nltk.corpus import stopwords # Import stop words
    from string import punctuation
    import re
    from PIL import Image
    from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
   
    data_path = r''
    file_name = 'IHMStefanini_industrial_safety_and_health_database_with_accidents_description.csv'
    output_file_name = 'capproj1.csv'

    #Import data

    accident_df_raw = pd.read_csv(data_path + file_name)
    accident_df_raw.head()

    #Data analysis and EDA as below

    #accident_df_raw.info() ****HERE ***

    accident_df_raw = accident_df_raw.drop(columns='Unnamed: 0', axis=1)

    accident_df_raw['Date'] = pd.to_datetime(accident_df_raw['Data'])
    accident_df_raw = accident_df_raw.drop(columns='Data', axis=1)
    #accident_df_raw.info() ****HERE ***

    accident_df_raw['Month'] =  accident_df_raw['Date'].apply(lambda d: d.month)
    accident_df_raw['Year'] =  accident_df_raw['Date'].apply(lambda d: d.year)
    accident_df_raw_data = accident_df_raw.copy()
    #print("Dataset has 425 rows and 12 columns ->", accident_df_raw.shape) ****HERE ***

    #print("Null entry count of dataset as below") ****HERE ***
    accident_df_raw.isnull().sum()

    #Makes count plot and prints value counts

    def make_count_plot(data, x, hue=" "):
        if(len(hue) <= 1):
            sns.countplot(x=x,data=data)
        else:
            fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))
            sns.countplot(x=x,data=data, ax=axes[0])
            sns.countplot(x=x,data=data,hue=hue, ax=axes[1])
        plt.show()
        
        print(data[x].value_counts())

    #Count plot of Accident level by country

    ## ****HERE ***
    # plt.figure(figsize=(15, 10)) 
    # make_count_plot(accident_df_raw, 'Countries', hue='Accident Level')

    #From below Country_01 has the highest level of accidents and level 1 accidents too

    #Count plot of Accident level by location

    ## ****HERE ***
    # plt.figure(figsize=(15, 10))
    # make_count_plot(accident_df_raw, 'Local', hue='Accident Level')

    #From below Local_03 has the highest level of accidents and level 1 accidents too

    #Count plot of Accident level by Industry Sector

    ## ****HERE ***
    #make_count_plot(accident_df_raw, 'Industry Sector', hue='Accident Level')

    #From below Mining has the highest level of accidents and level 1 accidents too

    #Count plot of Accident Level 

    ## ****HERE ***
    #make_count_plot(accident_df_raw, 'Accident Level')

    #From below level 1 accidents are most frequent

    #Count plot of Potential Accident Level

    ## ****HERE ***
    #make_count_plot(accident_df_raw, 'Potential Accident Level')

    #From below level 4 accidents are most Potential Accident Level

    #Count plot of Potential Accident Level wrt Accident Level

    ## ****HERE ***
    #make_count_plot(accident_df_raw, 'Potential Accident Level', hue='Accident Level')

    #From below level 4 accidents are most Potential Accident Level but level 1 accidents are high in occurrence

    #Count plot of Genre wrt Accident Level

    ## ****HERE ***
    #make_count_plot(accident_df_raw, 'Genre', hue='Accident Level')

    #From below Male Genre has the most accidents

    #Count plot of Employee or Third Party wrt Accident Level

    ## ****HERE ***
    # gender = { "Male": 0, "Female": 1 }
    # make_count_plot(accident_df_raw, 'Employee or Third Party', hue='Accident Level')

    #From below Third Party and Employee both have almost same accident occurrences with high occurrence in leve1 1 accidents

    #Count plot of Critical Risk

    ## ****HERE ***
    # plt.figure(figsize=(30, 15))
    # plt.xticks(rotation=30)
    # make_count_plot(accident_df_raw, 'Critical Risk')

    #From below Others have the most Critical Risk

    #Count plot of Accident Level by Month

    ## ****HERE ***
    # plt.figure(figsize=(15, 10))
    # make_count_plot(accident_df_raw, 'Month', hue='Accident Level')

    #From below Accidents mostly occur during the first 6 months of the year

    #Count plot of Accident Level by Year

    ## ****HERE ***
    # plt.figure(figsize=(15, 10))
    # make_count_plot(accident_df_raw, 'Year', hue='Accident Level')

    #2016 has had more accidents as compared to 2017

    #Chi-Square test below with alpha = 0.05
    #accident_df_raw_data = accident_df_raw
    accident_df_raw.columns

    accident_level = { "I": 0, "II": 1, "III": 2, "IV": 3, "V": 4, "VI": 5 }
    accident_df_raw["Potential Accident Level"] = accident_df_raw["Potential Accident Level"].map(accident_level)
    accident_df_raw["Accident Level"] = accident_df_raw["Accident Level"].map(accident_level)

    gender = { "Male": 0, "Female": 1 }
    accident_df_raw['Genre'] = accident_df_raw['Genre'].map(gender)

    countries_encoder = LabelEncoder()
    accident_df_raw['Countries'] = countries_encoder.fit_transform(accident_df_raw['Countries'])

    local_encoder = LabelEncoder()
    accident_df_raw['Local'] = local_encoder.fit_transform(accident_df_raw['Local'])

    industry_sector_encoder = LabelEncoder()
    accident_df_raw['Industry Sector'] = industry_sector_encoder.fit_transform(accident_df_raw['Industry Sector'])

    gender_encoder = LabelEncoder()
    accident_df_raw['Genre'] = gender_encoder.fit_transform(accident_df_raw['Genre'])

    tp_encoder = LabelEncoder()
    accident_df_raw['Employee or Third Party'] = tp_encoder.fit_transform(accident_df_raw['Employee or Third Party'])

    risk_encoder = LabelEncoder()
    accident_df_raw['Critical Risk'] = risk_encoder.fit_transform(accident_df_raw['Critical Risk'])

    accident_df_raw.head()

    # H0: Countries have no impact on Accident Level
    # Ha: Countries have impact on Accident Level

    countries_cross_tab = pd.crosstab(index = accident_df_raw['Countries'], columns = accident_df_raw['Accident Level'])
    chi2, p, dof, ex =chi2_contingency(countries_cross_tab)
    p

    #Since p is less than alpha = 0.05 we reject the null hypothesis

    # H0: Local have no impact on Accident Level
    # Ha: Local have impact on Accident Level

    local_cross_tab = pd.crosstab(index = accident_df_raw['Local'], columns = accident_df_raw['Accident Level'])
    chi2, p, dof, ex =chi2_contingency(local_cross_tab)
    p

    #Since p is greater than alpha = 0.05 we hold the null hypothesis

    # H0: Industry Sector have no impact on Accident Level
    # Ha: Industry Sector have impact on Accident Level

    sector_cross_tab = pd.crosstab(index = accident_df_raw['Industry Sector'], columns = accident_df_raw['Accident Level'])
    chi2, p, dof, ex =chi2_contingency(sector_cross_tab)
    p

    #Since p is greater than alpha = 0.05 we hold the null hypothesis

    # H0: Employee or Third Party have no impact on Accident Level
    # Ha: Employee or Third Party have impact on Accident Level

    tp_cross_tab = pd.crosstab(index = accident_df_raw['Employee or Third Party'], columns = accident_df_raw['Accident Level'])
    chi2, p, dof, ex =chi2_contingency(tp_cross_tab)
    p

    #Since p is greater than alpha = 0.05 we hold the null hypothesis

    #Data Processing

    # accident_df_raw_data.shape
    # accident_df_raw_data.info()
    accident_df_raw_data.head()

    #Lower Case words in the description column
    accident_df_raw_data.iloc[0,8:9]
    accident_df_raw_data["Description"] = accident_df_raw_data["Description"].str.lower()
    accident_df_raw_data.iloc[0,8:9]

    #Remove stop words and punctuations from description column
    stop_words = stopwords.words('english') + list(punctuation)

    def removestopwords(text):
        text = re.sub("\'", "", text) 
        text = re.sub("[^a-zA-Z]"," ",text) 
        text = ' '.join(text.split()) 
        text = text.lower() 
        no_stopword_text = [w for w in text.split() if not w in stop_words and not w.isdigit()]
        return ' '.join(no_stopword_text)

    accident_df_raw_data["Description"] = accident_df_raw_data["Description"].apply(lambda x: removestopwords(x))
    accident_df_raw_data.iloc[0,8:9]

    #Word Cloud

    #text = accident_df_raw_data.Description[0]
    text = " ".join(desc for desc in accident_df_raw_data.Description)
    # print ("There are {} words in the combination of all Description.".format(len(text))) ****HERE ***

    wordcloud = WordCloud().generate(text)
    ## ** HERE **
    # plt.imshow(wordcloud, interpolation='bilinear')
    # plt.axis("off")
    # plt.show()

    #Save data to csv to pass to model

    #accident_df_raw_data = accident_df_raw_data.drop(columns='Unnamed: 0', axis=1)
    accident_df_raw_data = accident_df_raw_data.drop(columns='Local', axis=1)
    accident_df_raw_data = accident_df_raw_data.drop(columns='Industry Sector', axis=1)
    accident_df_raw_data = accident_df_raw_data.drop(columns='Potential Accident Level', axis=1)
    accident_df_raw_data = accident_df_raw_data.drop(columns='Genre', axis=1)
    accident_df_raw_data = accident_df_raw_data.drop(columns='Employee or Third Party', axis=1)
    accident_df_raw_data = accident_df_raw_data.drop(columns='Critical Risk', axis=1)
    accident_df_raw_data = accident_df_raw_data.drop(columns='Date', axis=1)
    accident_df_raw_data = accident_df_raw_data.drop(columns='Month', axis=1)
    accident_df_raw_data = accident_df_raw_data.drop(columns='Year', axis=1)
    accident_df_raw_data.to_csv(data_path + output_file_name)
    accident_df_raw_data.head()
    return accident_df_raw_data

